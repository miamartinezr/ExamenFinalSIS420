{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "# utilizado para la manipulación de directorios y rutas\n",
        "import os\n",
        "\n",
        "# Cálculo científico y vectorial para python\n",
        "import numpy as np\n",
        "\n",
        "# Libreria para graficos\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Modulo de optimizacion en scipy\n",
        "from scipy import optimize\n",
        "\n",
        "# le dice a matplotlib que incruste gráficos en el cuaderno\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "MFZT8mWhb8JX"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "Ks98aGBF2pOJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combinacion de archivos CSV\n",
        "\n",
        "import csv\n",
        "\n",
        "input_files = ['0.csv', '1.csv', '2.csv', '3.csv'] #lista de archivos CSV para combinar\n",
        "output_file = 'nuevo_dataset.csv' #archivo resultante de la combinacion\n",
        "\n",
        "rows = [] #alamcena filas de los archivos CSV\n",
        "\n",
        "''' abrimos modo lectura con (open(file, 'r')) y creamos un objeto csv_reader para leer las filas del archivo CSV.\n",
        "las filas leidas se agregan a la lista (rows).'''\n",
        "for file in input_files:\n",
        "    with open(file, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        rows.extend(csv_reader)\n",
        "\n",
        "''' abrimos el archivo de salida en modo escritura ('w'), el parametro(newline='') se utiliza para manejar\n",
        "el comportamiento de nuevas lineas de diferentes plataformas. creamos (csv_reader) para escribir todas las filas\n",
        "almacenadas en (rows) en el archivo de salida'''\n",
        "with open(output_file, 'w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerows(rows)"
      ],
      "metadata": {
        "id": "IAOkAWV4JB1e"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carga de dataset\n",
        "data = pd.read_csv('nuevo_dataset.csv')\n",
        "print(data)\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2cfqctK5GXg",
        "outputId": "585b52cf-6c36-47f9-ef48-4af31a1c1d2a"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       26.0  4.0  5.0   8.0  -1.0  -13.0  -109.0  -66.0  -9.0  2.0  ...  \\\n",
            "0     -47.0 -6.0 -5.0  -7.0  13.0   -1.0    35.0  -10.0  10.0 -4.0  ...   \n",
            "1     -19.0 -8.0 -8.0  -8.0 -21.0   -6.0   -79.0   12.0   0.0  5.0  ...   \n",
            "2       2.0  3.0  0.0   2.0   0.0   22.0   106.0  -14.0 -16.0 -2.0  ...   \n",
            "3       6.0  0.0  0.0  -2.0 -14.0   10.0   -51.0    5.0   7.0  0.0  ...   \n",
            "4      15.0 -5.0 -5.0 -15.0  12.0  -22.0   -38.0   36.0   9.0  6.0  ...   \n",
            "...     ...  ...  ...   ...   ...    ...     ...    ...   ...  ...  ...   \n",
            "11672  -3.0 -1.0 -1.0  -1.0 -28.0   20.0     5.0    0.0  -5.0  0.0  ...   \n",
            "11673 -13.0 -5.0 -4.0  -3.0  -4.0  -24.0   -10.0   -8.0  20.0  9.0  ...   \n",
            "11674  -1.0 -3.0 -1.0   1.0  30.0   38.0    -1.0   36.0 -10.0  1.0  ...   \n",
            "11675   1.0  4.0  4.0   5.0   9.0  -10.0     4.0    1.0  -2.0 -1.0  ...   \n",
            "11676  -2.0  4.0  2.0  -4.0  12.0    3.0    -2.0    9.0  -8.0 -2.0  ...   \n",
            "\n",
            "       -28.0  61.0  4.0.3  8.0.1  5.0.1  4.0.4  -7.0.1  -59.0  16.0  0  \n",
            "0      -25.0  47.0    6.0    6.0    5.0   13.0    21.0  111.0  15.0  0  \n",
            "1      -83.0   7.0    7.0    1.0   -8.0    7.0    21.0  114.0  48.0  0  \n",
            "2      -38.0 -11.0    4.0    7.0   11.0   33.0    39.0  119.0  43.0  0  \n",
            "3       38.0 -35.0   -8.0    2.0    6.0  -13.0   -24.0 -112.0 -69.0  0  \n",
            "4      -26.0   5.0    6.0    6.0   11.0    5.0    30.0  -48.0  25.0  0  \n",
            "...      ...   ...    ...    ...    ...    ...     ...    ...   ... ..  \n",
            "11672   -3.0   1.0    4.0    3.0    4.0  -51.0   -49.0    5.0  -9.0  3  \n",
            "11673    6.0  -3.0   -3.0   -3.0   -5.0   -4.0   -45.0  -12.0 -15.0  3  \n",
            "11674   14.0  -8.0   -4.0   -4.0   -4.0  -21.0   -29.0   -5.0   0.0  3  \n",
            "11675  -16.0  -3.0    0.0   -3.0   -5.0  -36.0   -90.0    3.0   5.0  3  \n",
            "11676    2.0   1.0    0.0   -1.0   -2.0  -30.0    64.0   11.0   5.0  3  \n",
            "\n",
            "[11677 rows x 65 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11677 entries, 0 to 11676\n",
            "Data columns (total 65 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   26.0     11677 non-null  float64\n",
            " 1   4.0      11677 non-null  float64\n",
            " 2   5.0      11677 non-null  float64\n",
            " 3   8.0      11677 non-null  float64\n",
            " 4   -1.0     11677 non-null  float64\n",
            " 5   -13.0    11677 non-null  float64\n",
            " 6   -109.0   11677 non-null  float64\n",
            " 7   -66.0    11677 non-null  float64\n",
            " 8   -9.0     11677 non-null  float64\n",
            " 9   2.0      11677 non-null  float64\n",
            " 10  4.0.1    11677 non-null  float64\n",
            " 11  13.0     11677 non-null  float64\n",
            " 12  -18.0    11677 non-null  float64\n",
            " 13  -30.0    11677 non-null  float64\n",
            " 14  -119.0   11677 non-null  float64\n",
            " 15  -45.0    11677 non-null  float64\n",
            " 16  17.0     11677 non-null  float64\n",
            " 17  6.0      11677 non-null  float64\n",
            " 18  -1.0.1   11677 non-null  float64\n",
            " 19  -9.0.1   11677 non-null  float64\n",
            " 20  27.0     11677 non-null  float64\n",
            " 21  20.0     11677 non-null  float64\n",
            " 22  91.0     11677 non-null  float64\n",
            " 23  71.0     11677 non-null  float64\n",
            " 24  -26.0    11677 non-null  float64\n",
            " 25  -1.0.2   11677 non-null  float64\n",
            " 26  0.0      11677 non-null  float64\n",
            " 27  1.0      11677 non-null  float64\n",
            " 28  13.0.1   11677 non-null  float64\n",
            " 29  20.0.1   11677 non-null  float64\n",
            " 30  -62.0    11677 non-null  float64\n",
            " 31  -19.0    11677 non-null  float64\n",
            " 32  29.0     11677 non-null  float64\n",
            " 33  2.0.1    11677 non-null  float64\n",
            " 34  2.0.2    11677 non-null  float64\n",
            " 35  0.0.1    11677 non-null  float64\n",
            " 36  -23.0    11677 non-null  float64\n",
            " 37  -1.0.3   11677 non-null  float64\n",
            " 38  -80.0    11677 non-null  float64\n",
            " 39  4.0.2    11677 non-null  float64\n",
            " 40  -7.0     11677 non-null  float64\n",
            " 41  -6.0     11677 non-null  float64\n",
            " 42  -12.0    11677 non-null  float64\n",
            " 43  -27.0    11677 non-null  float64\n",
            " 44  -11.0    11677 non-null  float64\n",
            " 45  -16.0    11677 non-null  float64\n",
            " 46  -67.0    11677 non-null  float64\n",
            " 47  -8.0     11677 non-null  float64\n",
            " 48  -27.0.1  11677 non-null  float64\n",
            " 49  1.0.1    11677 non-null  float64\n",
            " 50  1.0.2    11677 non-null  float64\n",
            " 51  13.0.2   11677 non-null  float64\n",
            " 52  -8.0.1   11677 non-null  float64\n",
            " 53  -11.0.1  11677 non-null  float64\n",
            " 54  21.0     11677 non-null  float64\n",
            " 55  -28.0    11677 non-null  float64\n",
            " 56  61.0     11677 non-null  float64\n",
            " 57  4.0.3    11677 non-null  float64\n",
            " 58  8.0.1    11677 non-null  float64\n",
            " 59  5.0.1    11677 non-null  float64\n",
            " 60  4.0.4    11677 non-null  float64\n",
            " 61  -7.0.1   11677 non-null  float64\n",
            " 62  -59.0    11677 non-null  float64\n",
            " 63  16.0     11677 non-null  float64\n",
            " 64  0        11677 non-null  int64  \n",
            "dtypes: float64(64), int64(1)\n",
            "memory usage: 5.8 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.iloc[:, :64]\n",
        "y = data.iloc[:, 64]\n",
        "m = y.size\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(x)\n",
        "print('---'*30)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnIfzlO9d7xv",
        "outputId": "d0a82a51-1555-489a-e10e-e22e99a2227e"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11677, 64)\n",
            "(11677,)\n",
            "       26.0  4.0  5.0   8.0  -1.0  -13.0  -109.0  -66.0  -9.0  2.0  ...  \\\n",
            "0     -47.0 -6.0 -5.0  -7.0  13.0   -1.0    35.0  -10.0  10.0 -4.0  ...   \n",
            "1     -19.0 -8.0 -8.0  -8.0 -21.0   -6.0   -79.0   12.0   0.0  5.0  ...   \n",
            "2       2.0  3.0  0.0   2.0   0.0   22.0   106.0  -14.0 -16.0 -2.0  ...   \n",
            "3       6.0  0.0  0.0  -2.0 -14.0   10.0   -51.0    5.0   7.0  0.0  ...   \n",
            "4      15.0 -5.0 -5.0 -15.0  12.0  -22.0   -38.0   36.0   9.0  6.0  ...   \n",
            "...     ...  ...  ...   ...   ...    ...     ...    ...   ...  ...  ...   \n",
            "11672  -3.0 -1.0 -1.0  -1.0 -28.0   20.0     5.0    0.0  -5.0  0.0  ...   \n",
            "11673 -13.0 -5.0 -4.0  -3.0  -4.0  -24.0   -10.0   -8.0  20.0  9.0  ...   \n",
            "11674  -1.0 -3.0 -1.0   1.0  30.0   38.0    -1.0   36.0 -10.0  1.0  ...   \n",
            "11675   1.0  4.0  4.0   5.0   9.0  -10.0     4.0    1.0  -2.0 -1.0  ...   \n",
            "11676  -2.0  4.0  2.0  -4.0  12.0    3.0    -2.0    9.0  -8.0 -2.0  ...   \n",
            "\n",
            "        21.0  -28.0  61.0  4.0.3  8.0.1  5.0.1  4.0.4  -7.0.1  -59.0  16.0  \n",
            "0     -105.0  -25.0  47.0    6.0    6.0    5.0   13.0    21.0  111.0  15.0  \n",
            "1     -128.0  -83.0   7.0    7.0    1.0   -8.0    7.0    21.0  114.0  48.0  \n",
            "2      -54.0  -38.0 -11.0    4.0    7.0   11.0   33.0    39.0  119.0  43.0  \n",
            "3       60.0   38.0 -35.0   -8.0    2.0    6.0  -13.0   -24.0 -112.0 -69.0  \n",
            "4       22.0  -26.0   5.0    6.0    6.0   11.0    5.0    30.0  -48.0  25.0  \n",
            "...      ...    ...   ...    ...    ...    ...    ...     ...    ...   ...  \n",
            "11672   -3.0   -3.0   1.0    4.0    3.0    4.0  -51.0   -49.0    5.0  -9.0  \n",
            "11673    5.0    6.0  -3.0   -3.0   -3.0   -5.0   -4.0   -45.0  -12.0 -15.0  \n",
            "11674   12.0   14.0  -8.0   -4.0   -4.0   -4.0  -21.0   -29.0   -5.0   0.0  \n",
            "11675   -2.0  -16.0  -3.0    0.0   -3.0   -5.0  -36.0   -90.0    3.0   5.0  \n",
            "11676  -10.0    2.0   1.0    0.0   -1.0   -2.0  -30.0    64.0   11.0   5.0  \n",
            "\n",
            "[11677 rows x 64 columns]\n",
            "------------------------------------------------------------------------------------------\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "11672    3\n",
            "11673    3\n",
            "11674    3\n",
            "11675    3\n",
            "11676    3\n",
            "Name: 0, Length: 11677, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividir un conjunto de datos en conjuento de entrenamiento y prueba\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) #semilla para generacion de numeros aleatorios en la division"
      ],
      "metadata": {
        "id": "_K7Srdfze8wl"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion que devuelve cero si el valor de entrada es menor a cero, de lo contrario se mantiene el valor de entrada\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluPrime(x):\n",
        "  return x > 0"
      ],
      "metadata": {
        "id": "-SDn69rxhg0B"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FUNCION SOFTMAX.-***\n",
        "Utilizamos esta funcion para problemas de clasificacion multiclase.\n",
        "La funcion Softmax calcula la probabilidad relativa de cada clase en un conjunto de clases mutuamente excluyente, asegurandose que la suma de las probabilidades sea igual a 1."
      ],
      "metadata": {
        "id": "T_ts77ocjOH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
      ],
      "metadata": {
        "id": "FZ6VQo5Tixq2"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FUNCION CROSSENTROPY.-***\n",
        "Implementa el calculo de la entropia cruzada entre las etiquetas reales y las predicciones."
      ],
      "metadata": {
        "id": "8-hi74ldoKWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy (aplica softmax + cross entropy de manera estable) -> usada para clasificación multiclase\n",
        "def crossentropy(y, y_hat):\n",
        "    '''utilizando indexacion avanzada de NumPy selecciona los valores 'y_hat' correspondienetes a las\n",
        "    etiquetas reales 'y', (np.arange) genera un array con los indices de las filas 'y_hat' y 'y' se utiliza\n",
        "    como indices  de columna para seleccionar los elementos correspondientes '''\n",
        "    logits = y_hat[np.arange(len(y_hat)),y]\n",
        "    '''1. (entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))) calcula la diferencia de logaritmos entre\n",
        "    los valores seleccionados 'logits' y la suma de probabilidades de todas las clases.\n",
        "    2. (np.sum(np.exp(y_hat), axis=-1)) calcula la suma de valores de probabilidad a lo largo del ultimo eje,\n",
        "    que corresponde a las clases.\n",
        "    3.(np.exp(y_hat)) calcula la exponencial de cada elemento de 'y_hat', esto permite obtener los valores\n",
        "    de probabilidad asociados con cada clase.'''\n",
        "    entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))\n",
        "    '''(entropy.mean()) calcula la media de los valores de entropia para obtener un unico valor representativo\n",
        "    de la entropia cruzada en el conjunto de datos'''\n",
        "    return entropy.mean()"
      ],
      "metadata": {
        "id": "cGPNx2vKkRI4"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FUNCION DEL GRADIENTE.-***\n",
        "Calcula el gradiente de la funcion de perdida de entropia cruzada con respecto a las predicciones 'y_hat'."
      ],
      "metadata": {
        "id": "FSQFjF0kpODu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_crossentropy(y, y_hat):\n",
        "    '''crea un array de ceros con la misma forma 'y_hat' que se utiliza para almacenar los valores\n",
        "    de las etiquetas reales codificadas en forma de \"one-hot vectors\". Inicialmente, todos los\n",
        "    elementos del array son cero.'''\n",
        "    answers = np.zeros_like(y_hat)\n",
        "    '''establece los elemntos correspondientes a las etiquetas reales en '1',\n",
        "    codificando las etiquetas en \"one-hot vectors\". '''\n",
        "    answers[np.arange(len(y_hat)),y] = 1\n",
        "    '''realiza la operacion final para calcular el gradiente de la funcion de perdida de entropia\n",
        "    cruzada con respecto a las predicciones 'y_hat'.'''\n",
        "    return (- answers + softmax(y_hat)) / y_hat.shape[0]"
      ],
      "metadata": {
        "id": "cmcDrI2Uot2N"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clase base MLP\n",
        "\n",
        "class MLP():\n",
        "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
        "    # pesos de la capa 1\n",
        "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(D_in+H)),\n",
        "                                  size=(D_in, H)), np.zeros(H)\n",
        "    # pesos de la capa 2\n",
        "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(H+D_out)),\n",
        "                                  size=(H, D_out)), np.zeros(D_out)\n",
        "    self.ws = []\n",
        "    # función de pérdida y derivada\n",
        "    self.loss = loss\n",
        "    self.grad_loss = grad_loss\n",
        "    # función de activación\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # salida de la capa 1\n",
        "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
        "    self.h = relu(self.h_pre)\n",
        "    # salida del MLP\n",
        "    y_hat = np.dot(self.h, self.w2) + self.b2\n",
        "    return self.activation(y_hat)\n",
        "\n",
        "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
        "    batch_size = len(X) if batch_size == None else batch_size\n",
        "    batches = len(X) // batch_size\n",
        "    l = []\n",
        "    for e in range(1,epochs+1):\n",
        "        # Mini-Batch Gradient Descent\n",
        "        _l = []\n",
        "        for b in range(batches):\n",
        "            # batch de datos\n",
        "            x = X[b*batch_size:(b+1)*batch_size]\n",
        "            y = Y[b*batch_size:(b+1)*batch_size]\n",
        "            # salida del perceptrón\n",
        "            y_pred = self(x)\n",
        "            # función de pérdida\n",
        "            loss = self.loss(y, y_pred)\n",
        "            _l.append(loss)\n",
        "            # Backprop\n",
        "            dldy = self.grad_loss(y, y_pred)\n",
        "            grad_w2 = np.dot(self.h.T, dldy)\n",
        "            grad_b2 = dldy.mean(axis=0)\n",
        "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)\n",
        "            grad_w1 = np.dot(x.T, dldh)\n",
        "            grad_b1 = dldh.mean(axis=0)\n",
        "            # Update (GD)\n",
        "            self.w1 = self.w1 - lr * grad_w1\n",
        "            self.b1 = self.b1 - lr * grad_b1\n",
        "            self.w2 = self.w2 - lr * grad_w2\n",
        "            self.b2 = self.b2 - lr * grad_b2\n",
        "        l.append(np.mean(_l))\n",
        "        # guardamos pesos intermedios para visualización\n",
        "        self.ws.append((\n",
        "            self.w1.copy(),\n",
        "            self.b1.copy(),\n",
        "            self.w2.copy(),\n",
        "            self.b2.copy()\n",
        "        ))\n",
        "        if verbose and not e % log_each:\n",
        "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
        "\n",
        "  #aqui vemos que tal  esta la prediccion\n",
        "  def predict(self, ws, x):\n",
        "    w1, b1, w2, b2 = ws\n",
        "    h = relu(np.dot(x, w1) + b1)\n",
        "    y_hat = np.dot(h, w2) + b2\n",
        "    return self.activation(y_hat)"
      ],
      "metadata": {
        "id": "QnAHZuuOsUFI"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP para clasificación multiclase\n",
        "class MLPClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, crossentropy, grad_crossentropy, linear)"
      ],
      "metadata": {
        "id": "rcLxoWKGtLZl"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPClassification(D_in=64, H=50, D_out=4)\n",
        "epochs, lr = 300, 0.01\n",
        "\n",
        "# normalización datos\n",
        "x_mean, x_std = x.mean(axis=0), x.std(axis=0)\n",
        "x_norm = (x_train - x_mean) / x_std\n",
        "\n",
        "model.fit(x_norm, y_train, epochs, lr, batch_size=1, log_each=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAl5TTaEuDP1",
        "outputId": "fd5922b7-04c8-474d-e082-22820e95790d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/300, Loss: 0.19558\n",
            "Epoch: 20/300, Loss: 0.10788\n",
            "Epoch: 30/300, Loss: 0.07321\n",
            "Epoch: 40/300, Loss: 0.05547\n",
            "Epoch: 50/300, Loss: 0.04469\n",
            "Epoch: 60/300, Loss: 0.03744\n",
            "Epoch: 70/300, Loss: 0.03223\n",
            "Epoch: 80/300, Loss: 0.02831\n",
            "Epoch: 90/300, Loss: 0.02524\n",
            "Epoch: 100/300, Loss: 0.02278\n",
            "Epoch: 110/300, Loss: 0.02076\n",
            "Epoch: 120/300, Loss: 0.01907\n",
            "Epoch: 130/300, Loss: 0.01763\n",
            "Epoch: 140/300, Loss: 0.01640\n",
            "Epoch: 150/300, Loss: 0.01533\n",
            "Epoch: 160/300, Loss: 0.01440\n",
            "Epoch: 170/300, Loss: 0.01357\n",
            "Epoch: 180/300, Loss: 0.01283\n",
            "Epoch: 190/300, Loss: 0.01217\n",
            "Epoch: 200/300, Loss: 0.01157\n",
            "Epoch: 210/300, Loss: 0.01104\n",
            "Epoch: 220/300, Loss: 0.01054\n",
            "Epoch: 230/300, Loss: 0.01010\n",
            "Epoch: 240/300, Loss: 0.00968\n",
            "Epoch: 250/300, Loss: 0.00930\n",
            "Epoch: 260/300, Loss: 0.00895\n",
            "Epoch: 270/300, Loss: 0.00863\n",
            "Epoch: 280/300, Loss: 0.00833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# últimos pesos encontrados\n",
        "w = model.ws[-1] #extre el ultimo elemento de la lista 'model-ws' y lo asigna a la variable w\n",
        "w"
      ],
      "metadata": {
        "id": "sHV9HZU0w2gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los primeros 5 elementos de los datos de prueba\n",
        "print(\"Primeros 5 datos de entrada de prueba (X_test):\")\n",
        "print(x_test[:5])\n",
        "\n",
        "print(\"Primeras 5 etiquetas de prueba (y_test):\")\n",
        "print(y_test[:5])"
      ],
      "metadata": {
        "id": "RB36ppMfxWj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nuevo punto\n",
        "x_new = x_test\n",
        "X_new = [ -7.0, 0.0, -2.0, -5.0, -9.0, 15.0, 0.0, -4.0, 0.0, -3.0, -2.0, -3.0, 1.0, 7.0, 0.0, 4.0, 7.0, 0.0, -1.0, -7.0, -24.0, -1.0, -3.0, -4.0, -6.0, -1.0, 1.0, 5.0, 52.0, 29.0, -1.0, 0.0, -30.0, 0.0, 1.0, 3.0, -24.0, -23.0, -1.0, -4.0, 38.0, 3.0, 0.0, -2.0, -12.0, -21.0, 0.0, 7.0, 30.0, 0.0, -1.0, 0.0, -28.0, -8.0, 0.0, 5.0, -2.0, -2.0, -4.0, -4.0, 7.0, 11.0, 3.0, 20.0]\n",
        "y_pred = model.predict(w, X_new)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "BECVh6CkyE7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, x, t = 0.5):\n",
        "    w = model.ws[-1]\n",
        "    x = np.c_[np.ones(len(x)), x]\n",
        "    y = model(w, x)\n",
        "    return (y > t).astype(np.int)"
      ],
      "metadata": {
        "id": "4pqpPFxbz8Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y):\n",
        "    return np.sum(y_pred == y_test) / len(y)"
      ],
      "metadata": {
        "id": "qLyyOOhOz_oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y_train))\n",
        "print(len(y_test))"
      ],
      "metadata": {
        "id": "_1hRE9T10Blg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "a_yiiap30C6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = MLPClassification(D_in=64, H=2, D_out=4)\n",
        "epochs, lr = 300, 0.01"
      ],
      "metadata": {
        "id": "KGvs5Wh50EQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importacion de funciones\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "'''crea un objeto MLPClassifier, le pasamos dos capas ocultas (30,20)'''\n",
        "y_pred_classifier = MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=300)\n",
        "\n",
        "'''ajusta el clasificador a los datos de entrenamiento, esto  realiza el proceso de\n",
        "entrenamiento de la red neuronal utilizab¿ndo los datos de entrenamiento y prueba'''\n",
        "y_pred_classifier.fit(x_train, y_train)\n",
        "\n",
        "'''aqui realizamos la prediccion utilizando  'y_pred_classifier' en los datos de prueba\n",
        "'x_test', lafuncion 'predict' se utiliza para generar tiquetas predichas para los datos de prueba'''\n",
        "y_pred_array = y_pred_classifier.predict(x_test)\n",
        "''' 'accuracy_score' calcula la presicion comparando etiquetas verdaderas de 'y_test' con las etiquetas\n",
        "predichas 'y_pred_array'. el resultado se multiplica por 100 para obtener el porcentaje de prediccion '''\n",
        "accuracy_percentage = accuracy_score(y_test, y_pred_array) * 100\n",
        "print(accuracy_percentage)\n"
      ],
      "metadata": {
        "id": "10HOl-rT0IYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}